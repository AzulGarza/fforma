{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross data matrix methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Methodology for high-dimensional and low-sample-size data (HDLSS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References\n",
    "- https://www.sciencedirect.com/science/article/pii/S0047259X19301915 (Main reference, in particular 2.2. Cross data matrix method)\n",
    "- https://www.sciencedirect.com/science/article/pii/S0047259X10000904\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from math import sqrt\n",
    "from copy import deepcopy\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.utils.extmath import svd_flip\n",
    "from statsmodels.regression.quantile_regression import QuantReg\n",
    "\n",
    "from src.meta_results_r_data import prepare_fforma_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.set_printoptions(precision=3, suppress=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array([[7827.764, 8421.74 , 8422.   , 6535.873, 8750.611, 8422.   ,\n",
    "        7827.764, 8577.088, 8487.396],\n",
    "       [7333.335, 8421.74 , 8422.   , 6471.836, 9079.222, 8422.   ,\n",
    "        7333.335, 8577.088, 8553.045],\n",
    "       [6921.949, 8421.74 , 8422.   , 6462.348, 9407.833, 8422.   ,\n",
    "        6921.949, 8577.088, 8618.694],\n",
    "       [6579.659, 8421.74 , 8422.   , 6460.832, 9736.444, 8422.   ,\n",
    "        6579.659, 8577.088, 8684.343]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = np.array([[16037.273, 16661.017, 15449.   , 21518.987, 16037.273, 15449.   ,\n",
    "        16778.538, 15916.413, 15620.592],\n",
    "       [16625.545, 17873.283, 15449.   , 29255.582, 16625.545, 15449.   ,\n",
    "        16961.09 , 15916.413, 15792.501],\n",
    "       [17213.818, 19085.549, 15449.   , 32706.167, 17213.818, 15449.   ,\n",
    "        16335.008, 15916.413, 15964.41 ],\n",
    "       [17802.091, 20297.815, 15449.   , 33172.966, 17802.091, 15449.   ,\n",
    "        15752.033, 15916.413, 16136.319]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(4, 9)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Issues with PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_or = PCA().fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.615,  0.   , -0.   ,  0.036, -0.484, -0.   ,  0.615, -0.   ,\n",
       "        -0.097],\n",
       "       [-0.296,  0.   , -0.   , -0.478, -0.757, -0.   , -0.296, -0.   ,\n",
       "        -0.151],\n",
       "       [-0.186,  0.   , -0.   ,  0.878, -0.393, -0.   , -0.186, -0.   ,\n",
       "        -0.078],\n",
       "       [-0.05 , -0.004,  0.001, -0.   ,  0.195,  0.   ,  0.05 ,  0.   ,\n",
       "        -0.978]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_or.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1063.657,  -28.976,    1.354,    0.   ],\n",
       "       [ 288.33 ,   35.398,   -4.891,    0.   ],\n",
       "       [-382.96 ,   24.557,    5.797,    0.   ],\n",
       "       [-969.026,  -30.978,   -2.26 ,    0.   ]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_or.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7935.364, -18866.516,   6531.23 ,  -5548.782],\n",
       "       [  8385.266, -23265.036,  12932.431,  -5627.267],\n",
       "       [  8184.59 , -25375.126,  15723.   ,  -5745.786],\n",
       "       [  7903.506, -26071.375,  15887.07 ,  -5862.171]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca_or.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### New methodology"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_wm = X - X.mean(0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X1, X2 = np.split(X_wm, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X1 shape (2, 9)\n",
      "X2 shape (2, 9)\n"
     ]
    }
   ],
   "source": [
    "print('X1 shape', X1.shape)\n",
    "print('X2 shape', X2.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "n1, n2 = X1.shape[0], X2.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "S1 = X1 @ X2.T / sqrt(n1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S1 shape (2, 2)\n"
     ]
    }
   ],
   "source": [
    "print('S1 shape', S1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting eigenvalues and eigenvectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "u1, lambdas, u2 = np.linalg.svd(S1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting PCs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "h1 = X1.T @ u1 / np.sqrt(n1 * lambdas)\n",
    "h2 = X2.T @ u2 / np.sqrt(n2 * lambdas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = 0.5 * (h1 + h2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "h = h / np.linalg.norm(h, axis=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pc_cross_data = X_wm @ h"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA train:\n",
      " [[1063.657  -28.976    1.354    0.   ]\n",
      " [ 288.33    35.398   -4.891    0.   ]\n",
      " [-382.96    24.557    5.797    0.   ]\n",
      " [-969.026  -30.978   -2.26     0.   ]]\n",
      "Cross-data-matrix train:\n",
      " [[-1063.642   -31.923]\n",
      " [ -288.347    34.514]\n",
      " [  382.948    25.711]\n",
      " [  969.042   -28.302]]\n",
      "PCA test:\n",
      " [[  7935.364 -18866.516   6531.23   -5548.782]\n",
      " [  8385.266 -23265.036  12932.431  -5627.267]\n",
      " [  8184.59  -25375.126  15723.     -5745.786]\n",
      " [  7903.506 -26071.375  15887.07   -5862.171]]\n",
      "Cross-data-matrix test:\n",
      " [[ -7925.748 -18787.896]\n",
      " [ -8373.388 -23090.555]\n",
      " [ -8171.628 -25157.723]\n",
      " [ -7890.19  -25850.629]]\n"
     ]
    }
   ],
   "source": [
    "print('PCA train:\\n', pca_or.transform(X))\n",
    "print('Cross-data-matrix train:\\n', pc_cross_data)\n",
    "print('PCA test:\\n', pca_or.transform(X_test))\n",
    "print('Cross-data-matrix test:\\n', (X_test - X.mean(0)) @ h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rotation PCA:\n",
      " [[ 0.615  0.    -0.     0.036 -0.484 -0.     0.615 -0.    -0.097]\n",
      " [-0.296  0.    -0.    -0.478 -0.757 -0.    -0.296 -0.    -0.151]\n",
      " [-0.186  0.    -0.     0.878 -0.393 -0.    -0.186 -0.    -0.078]\n",
      " [-0.05  -0.004  0.001 -0.     0.195  0.     0.05   0.    -0.978]]\n",
      "Rotation cross-data-matrix:\n",
      " [[-0.614  0.     0.    -0.036  0.484  0.    -0.614  0.     0.097]\n",
      " [-0.3    0.     0.    -0.465 -0.762  0.    -0.3    0.    -0.152]]\n"
     ]
    }
   ],
   "source": [
    "print('Rotation PCA:\\n', pca_or.components_)\n",
    "print('Rotation cross-data-matrix:\\n', h.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FQRA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CrossDataMatrix class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CrossDataMatrix:\n",
    "    \n",
    "    def __init__(self, n_components):\n",
    "        self.n_components = n_components\n",
    "    \n",
    "    def _fit(self, X, n_components):\n",
    "        # Center data\n",
    "        X = deepcopy(X) # Avoid overwrite original X\n",
    "        self.mean_ = np.mean(X, axis=0)\n",
    "        X -= self.mean_\n",
    "        \n",
    "        X1, X2 = np.split(X, 2)\n",
    "        n1, n2 = X1.shape[0], X2.shape[0]\n",
    "        \n",
    "        S1 = X1 @ X2.T / sqrt(n1)\n",
    "        \n",
    "        u1, lambdas, u2 = np.linalg.svd(S1)\n",
    "\n",
    "        h1 = X1.T @ u1 / np.sqrt(n1 * lambdas)\n",
    "        h2 = X2.T @ u2 / np.sqrt(n2 * lambdas)\n",
    "        \n",
    "        h = 0.5 * (h1 + h2)\n",
    "        h = h / np.linalg.norm(h, axis=0)\n",
    "        \n",
    "        self.components_ = h[:, :n_components]\n",
    "        \n",
    "        return h\n",
    "    \n",
    "    def fit(self, X):\n",
    "        self._fit(X, self.n_components)\n",
    "        \n",
    "        return self\n",
    "    \n",
    "    def transform(self, X):\n",
    "        X = deepcopy(X)\n",
    "        if self.mean_ is not None:\n",
    "            X -= self.mean_\n",
    "        X_transformed = X @ self.components_\n",
    "        \n",
    "        return X_transformed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = CrossDataMatrix(n_components=1).fit(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1063.642],\n",
       "       [ -288.347],\n",
       "       [  382.948],\n",
       "       [  969.042]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pca.transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "% of missing values, train set\n",
      "x_acf10               7.551487\n",
      "diff1_acf1            7.551487\n",
      "diff1_acf10           7.551487\n",
      "diff2_acf1            7.551487\n",
      "diff2_acf10           8.543097\n",
      "seas_acf1            39.511823\n",
      "arch_lm               9.687262\n",
      "garch_acf             0.076278\n",
      "arch_r2              13.196034\n",
      "garch_r2             13.348589\n",
      "nonlinearity          1.144165\n",
      "x_pacf5               1.144165\n",
      "diff1x_pacf5          1.830664\n",
      "diff2x_pacf5          2.212052\n",
      "seas_pacf            39.511823\n",
      "e_acf10               7.551487\n",
      "seasonal_strength    39.511823\n",
      "peak                 39.511823\n",
      "trough               39.511823\n",
      "hw_alpha             39.511823\n",
      "hw_beta              39.511823\n",
      "hw_gamma             39.511823\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "% of missing values, test set\n",
      "x_acf10               1.830664\n",
      "diff1_acf1            1.830664\n",
      "diff1_acf10           2.212052\n",
      "diff2_acf1            2.212052\n",
      "diff2_acf10           7.551487\n",
      "seas_acf1            39.511823\n",
      "arch_lm               7.551487\n",
      "arch_r2               7.551487\n",
      "garch_r2              7.551487\n",
      "diff2x_pacf5          1.144165\n",
      "seas_pacf            39.511823\n",
      "e_acf10               1.830664\n",
      "seasonal_strength    39.511823\n",
      "peak                 39.511823\n",
      "trough               39.511823\n",
      "hw_alpha             39.511823\n",
      "hw_beta              39.511823\n",
      "hw_gamma             39.511823\n",
      "dtype: float64\n",
      "\n",
      "\n",
      "Replacing with zeros...\n"
     ]
    }
   ],
   "source": [
    "data = prepare_fforma_data(directory='data', dataset_name=None, kind='TOURISM')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data['preds_train_df']\n",
    "y_train = data['y_train_df']\n",
    "X_test = data['preds_test_df']\n",
    "y_test = data['y_test_df']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FQRA using CrossDataMatrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA for HDLSS: 30.36310792719511\n",
      "Note: omitting normalization of h inside CrossDataMatrix gets ~29.\n"
     ]
    }
   ],
   "source": [
    "unique_ids = X_train['unique_id'].unique()\n",
    "smape_list = []\n",
    "y_hat_list = []\n",
    "u_id_list = []\n",
    "ds_list = []\n",
    "params = []\n",
    "for u_id in unique_ids:\n",
    "    x_id = X_train[X_train['unique_id']==u_id].drop(columns=['unique_id','ds']).values\n",
    "    x_test = X_test[X_test['unique_id']==u_id].drop(columns=['unique_id','ds']).values\n",
    "    y_id = y_train[y_train['unique_id']==u_id]['y'].values\n",
    "    y_test_id = y_test[y_test['unique_id']==u_id]['y'].values\n",
    "    \n",
    "    pca_model = CrossDataMatrix(n_components=1).fit(x_id)\n",
    "    X = pca_model.transform(x_id)\n",
    "    X = np.hstack([X, np.ones((len(X),1))])\n",
    "\n",
    "    reg = QuantReg(y_id, X).fit(0.5)\n",
    "    params.append(reg.params)\n",
    "\n",
    "    x_test_pca = pca_model.transform(x_test)\n",
    "    x_test_pca = np.hstack([x_test_pca, np.ones((len(x_test_pca),1))])\n",
    "    y_hat = reg.predict(x_test_pca)\n",
    "\n",
    "    my_smape=200*np.mean(np.abs(y_hat-y_test_id)/(np.abs(y_hat)+np.abs(y_test_id)))\n",
    "    smape_list.append(my_smape)\n",
    "    y_hat_list += y_hat.tolist()\n",
    "    u_id_list += [u_id]*len(y_hat)\n",
    "    ds_list += list(range(len(y_hat)))\n",
    "    \n",
    "df_hat = pd.DataFrame(list(zip(u_id_list, ds_list, y_hat_list)), \n",
    "               columns =['unique_id', 'ds', 'y_hat']) \n",
    "    \n",
    "print('PCA for HDLSS:', np.mean(smape_list))\n",
    "print('Note: omitting normalization of h inside CrossDataMatrix gets ~29.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FQRA using PCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PCA 30.24614997941319\n"
     ]
    }
   ],
   "source": [
    "unique_ids = X_train['unique_id'].unique()\n",
    "smape_list = []\n",
    "y_hat_list = []\n",
    "u_id_list = []\n",
    "ds_list = []\n",
    "for u_id in unique_ids:\n",
    "    x_id = X_train[X_train['unique_id']==u_id].drop(columns=['unique_id','ds']).values\n",
    "    x_test = X_test[X_test['unique_id']==u_id].drop(columns=['unique_id','ds']).values\n",
    "    y_id = y_train[y_train['unique_id']==u_id]['y'].values\n",
    "    y_test_id = y_test[y_test['unique_id']==u_id]['y'].values\n",
    "       \n",
    "    pca_model = PCA(n_components=1).fit(x_id)\n",
    "    X = pca_model.transform(x_id)\n",
    "    X = np.hstack([X, np.ones((len(X),1))])\n",
    "\n",
    "    reg = QuantReg(y_id, X).fit(0.5)\n",
    "\n",
    "    x_test_pca = pca_model.transform(x_test)\n",
    "    x_test_pca = np.hstack([x_test_pca, np.ones((len(x_test_pca),1))])\n",
    "    y_hat = reg.predict(x_test_pca)\n",
    "\n",
    "    my_smape=200*np.mean(np.abs(y_hat-y_test_id)/(np.abs(y_hat)+np.abs(y_test_id)))\n",
    "    smape_list.append(my_smape)\n",
    "    y_hat_list += y_hat.tolist()\n",
    "    u_id_list += [u_id]*len(y_hat)\n",
    "    ds_list += list(range(len(y_hat)))\n",
    "    \n",
    "df_hat = pd.DataFrame(list(zip(u_id_list, ds_list, y_hat_list)), \n",
    "               columns =['unique_id', 'ds', 'y_hat']) \n",
    "    \n",
    "print('PCA', np.mean(smape_list))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
